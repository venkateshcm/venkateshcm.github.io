<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Testing | Venkatesh CM]]></title>
  <link href="http://venkateshcm.github.io/blog/categories/testing/atom.xml" rel="self"/>
  <link href="http://venkateshcm.github.io/"/>
  <updated>2014-05-10T06:12:35+05:30</updated>
  <id>http://venkateshcm.github.io/</id>
  <author>
    <name><![CDATA[Venkatesh CM]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[How To Determine Web Application Thread Pool Size]]></title>
    <link href="http://venkateshcm.github.io/2014/05/How-To-Determine-Web-Applications-Thread-Poll-Size/"/>
    <updated>2014-05-10T00:00:00+05:30</updated>
    <id>http://venkateshcm.github.io/2014/05/How-To-Determine-Web-Applications-Thread-Poll-Size</id>
    <content type="html"><![CDATA[<p>Continuing on <a href="/2014/05/Architecture-Issues-Scaling-Web-Applications/">Architectural Issues faced while scaling web applications</a>, in this blog I will cover a common issue, how to determine web application thread pool size?, that shows up while deploying web applications to production or while performance testing web applications.</p>

<p><b><i>Thread Pool</i></b></p>

<p>In web applications thread pool size determines the number of concurrent requests that can be handled at any given time. If a web application gets more than thread pool size requests, they are either queued or rejected.</p>

<p>Please note concurrent is not same as parallel. Concurrent requests are number of requests being processed while only few of them could be running on CPUs at any point of time. Parallel requests are number of requests being processed while all of them are running on CPUs at any point of time.</p>

<p>In Non-blocking IO applications such as NodeJS, a single thread (process) can handles multiple requests concurrently. In multi-core CPUs boxes, parallel requests can be handled by increasing number of threads or processes.</p>

<p>In blocking IO applications such as Java SpringMVC, a single thread can handle only one request concurrently. To handle more than one request concurrently we have to increase the number of threads.</p>

<p><b><i>CPU Bound Applications</i></b></p>

<p> In CPU bound applications thread Pool size should be equal number of cpus on the box. Adding more threads would interrupt request processing due to thread context switching and also increases response time.</p>

<p> Non-blocking IO applications will be CPU bound as there are no thread wait time while requests get processed.</p>

<p><b><i>IO Bound Applications</i></b></p>

<p>Determining thread pool size of IO bound application is lot more complicated and depends on response time of down stream systems, since a thread is blocked until other system responds. We would have to increase the number of threads to better utilise CPU as discussed in <a href="/2014/04/Reactor-Pattern-Part-1-Non-blocking-I-O/">Reactor Pattern Part 1 : Applications with Blocking I/O</a>.</p>

<p><b><i>Little&rsquo;s law</i></b></p>

<p>Little&rsquo;s law was used in non technology fields like banks to figure out number of bank teller counters required to handle incoming bank customers.</p>

<p><b><a href="http://en.wikipedia.org/wiki/Little's_law">Little&rsquo;s law</a></b></p>

<pre><code>The long-term average number of customers in a stable system L is equal to the long-term 
average effective arrival rate, λ, multiplied by the average time a customer spends 
in the system, W; or expressed algebraically: 
L = λW.
</code></pre>

<p><b>Little&rsquo;s law applied to web applications</b></p>

<pre><code>The average number of threads in a system (Threads) is equal average web request 
arrival rate (WebRequests per sec), multiplied by the average response time (ResponseTime)
</code></pre>

<p>Threads = Number of Threads <br/>
WebRequests per sec = Number of Web Requests that can be processed in one second <br/>
ResponseTime = Time taken to process one web request <br/></p>

<pre><code>Threads = (WebRequests/sec) X ResponseTime
</code></pre>

<p>While the above equation provides the number of threads required to handle incoming requests, it does not provide information on the threads to cpu ratio i.e. how many threads should be allocated on a given box with x CPUs.</p>

<p><b><i>Testing to determining Thread Pool size</i></b></p>

<p>To find right thread pool size is to balance between throughput and response time. Starting with minimum a thread per cpu (Threads Pool Size = No of CPUs) , application thread pool size is directly proportional to the average response time of down stream systems until CPU usage is maxed out or response time is not degraded.</p>

<p>Below diagrams illustrate how number of requests, CPU and Response Time metrics are connected.</p>

<p>CPU Vs Number of Requests graph shows how CPU usage while increasing load on the web applications.</p>

<p>Response Time Vs Number of Requests graph shows response time impact due to increasing load on the web applications.</p>

<p>Green dot indicates point of optimal throughput and response time.</p>

<p><b>Thread pool size = Number of CPUs</b></p>

<p><img class="article-img" border="0" height="325" width="480" src="http://venkateshcm.github.io/img/blog/MinimumThreads.png" class="" style="display: inline-block;"></p>

<p>Above diagram depicts blocking IO bound applications when number of threads is equal to number cpus.
Application threads get blocked waiting for down stream systems to respond. Response time increases as requests get queued since threads are blocked. Application starts rejecting requests as all threads are blocked even though CPU usage is very low.</p>

<p><b>Large Thread pool size</b></p>

<p><img class="article-img" border="0" height="325" width="480" src="http://venkateshcm.github.io/img/blog/MaximumThreads.png" class="" style="display: inline-block;"></p>

<p>Above diagram depicts blocking IO bound applications when large number of threads are created in web application. Due to large number of threads, thread context switching will be very frequent. Application CPU usage gets maxed out even though throughput has not increased due to unnecessary thread context switching. Response Time degrades since requests are interrupted with context switching.</p>

<p><b>Optimal Thread pool size</b></p>

<p><img class="article-img" border="0" height="325" width="480" src="http://venkateshcm.github.io/img/blog/OptimalThreads.png" class="" style="display: inline-block;"></p>

<p>Above diagram depicts blocking IO bound applications when optimal number of threads are created in web application. CPU gets efficiently used with good throughput and fewer thread context switching. We notice good response time due to efficient request processing with fewer interrupts (context switching).</p>

<p><b><i>Thread Pool isolation</i></b></p>

<p>In most web applications, few types of web request take much longer to process than other web request types.The slower web requests might hog all threads and bring down entire application.</p>

<p>Couple of ways to handle this issue is</p>

<ul>
<li>to have separate box to handle slow web requests.</li>
<li>to allocate a separate thread pool for slow web requests within the same application.</li>
</ul>


<p>Determining optimal thread pool size of a blocking IO web application is difficult task. Usually done by performing several performance runs. Having several thread pools in a web application further complicates the process of determining optimal thread pool size.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Architecture Issues Scaling Web Applications]]></title>
    <link href="http://venkateshcm.github.io/2014/05/Architecture-Issues-Scaling-Web-Applications/"/>
    <updated>2014-05-05T00:00:00+05:30</updated>
    <id>http://venkateshcm.github.io/2014/05/Architecture-Issues-Scaling-Web-Applications</id>
    <content type="html"><![CDATA[<p>I will cover architecture issues that show up while scaling and performance tuning large scale web application in this blog.</p>

<p>Lets start by defining few terms to create common understanding and vocabulary. Later on I will go through different issues that pop-up while scaling web application like</p>

<ul>
<li>Architecture bottlenecks</li>
<li>Scaling Database</li>
<li>CPU Bound Application</li>
<li>IO Bound Application</li>
</ul>


<p><a href="/2014/05/How-To-Determine-Web-Applications-Thread-Poll-Size/">Determining optimal thread pool size of an web application</a> will be covered in next blog.</p>

<p><b><i>Performance</i></b></p>

<p>Term performance of web application is used to mean several things. Most developers are primarily concerned with are response time and scalability.</p>

<ul>
<li><p><h5>Response Time</h5></p>

<p>Is the time taken by web application to process request and return response. Applications should respond to requests (response time) within acceptable duration. If application is taking beyond the acceptable time, it is said to be non-performing or degraded.</p></li>
<li><p><h5>Scalability</h5></p>

<p>The web application is said to be scalable if by adding more hardware, application can linearly take more requests than before. Two ways of adding more hardware are</p>

<ul>
<li><b>Scaling Up (vertical scaling)</b> :&ndash;
increasing the number CPUs or adding faster CPUs on a single box.</li>
<li><b>Scaling Out (horizontal scaling)</b> :&ndash;
increasing the number of boxes.</li>
</ul>
</li>
</ul>


<p><b><i>Scaling Up Vs Scaling Out</i></b></p>

<p>Scaling out is considered more important as commodity hardware is cheaper compared to cost of special configuration hardware (super computer). But increasing the number of requests that an application can handle on a single commodity hardware box is also important. An application is said to be performing well if it can handle more requests with-out degrading response time by just adding more resources.</p>

<p><b><i>Response time Vs Scalability</i></b></p>

<p>Response time and Scalability don&rsquo;t aways go together i.e. application might have acceptable response times but can not handle more than certain number of requests or application is handle increasing number of requests but has poor or long response times. We have strike a balance between scalability and response time to get good performance of the application.</p>

<p><b><i>Capacity Planning</i></b></p>

<p>Capacity planning is an exercise of figuring out the required hardware to handle expected load in production. Usually it involves figuring out performance of application with fewer boxes and based on performance per box projecting it. Finally verifying it with load/performance tests.</p>

<p><b><i>Scalable Architecture</i></b></p>

<p>Application architecture is scalable if each layer in multi layered architecture is scalable (scale out). For example :&ndash; As shown in diagram below we should be able linearly scale by add additional box in Application Layer or Database Layer.</p>

<p><img class="article-img" border="0" height="325" width="480" src="http://venkateshcm.github.io/img/blog/scaling.png" class="" style="display: inline-block;"></p>

<p><b><i>Scaling Load Balancer</i></b></p>

<p>Load balancers can be scaled out by point DNS to multiple IP addresses and using DNS Round Robin for IP address lookup. Other option is to front another load balancer which distributes load to next level load balancers.</p>

<p>Adding multiple Load balancers is rare as a single box running nginx or HAProxy can handle more than 20K concurrent connections per box compared to web application boxes which can handle few thousand concurrent requests. So a single load balancer box can handle several web application boxes.</p>

<p><b><i>Scaling Database</i></b></p>

<p>Scaling database is one of the most common issues faced. Adding business logic (stored procedure, functions) in database layer brings in additional overhead and complexity.</p>

<p><b>RDBMS</b></p>

<p>RDBMS database can be scaled by having master-slave mode with read/writes on master database and only reads on slave databases. Master-Slave provides limited scaling of reads beyond which developers has to split the database into multiple databases.</p>

<p><b>NoSQL</b></p>

<p><a href="http://en.wikipedia.org/wiki/CAP_theorem">CAP theorem</a> has shown that is not possible to get Consistency, Availability and Partition tolerance simultaneously. NoSql databases usually compromise on consistency to get high availability and partition.</p>

<p><b>Splitting database</b></p>

<p>Database can be split vertically (Partitioning) or horizontally (Sharding).</p>

<ul>
<li><p>Vertically splitting (Partitioning) :&ndash; Database can be split into multiple loosely coupled sub-databases based of domain concepts. Eg:&ndash; Customer database, Product Database etc. Another way to split database is by moving few columns of an entity to one database and few other columns to another database. Eg:&ndash; Customer database , Customer contact Info database, Customer Orders database etc.</p></li>
<li><p>Horizontally splitting (Sharding) :&ndash; Database can be horizontally split into multiple database based on some discrete attribute. Eg:&ndash;  American Customers database, European Customers database.</p></li>
</ul>


<p>Transiting from single database to multiple database using partitioning or sharding is a challenging task.</p>

<p><b><i>Architecture bottlenecks</i></b></p>

<p>Scaling bottlenecks are formed due to two issues</p>

<ul>
<li><p><b>Centralised component</b>
A component in application architecture which can not be scaled out adds an upper limit on number of requests that entire architecture or request pipeline can handle.</p></li>
<li><p><b>High latency component</b>
A slow component in request pipeline puts lower limit on the response time of the application. Usual solution to fix this issue is to make high latency components into background jobs or executing them asynchronously with queuing.</p></li>
</ul>


<p><b><i>CPU Bound Application</i></b></p>

<p>An application is said to be CPU bound if application throughput is limited by its CPU. By increasing CPU speed application response time can be reduced.</p>

<p>Few scenarios where applications could be CPU Bound</p>

<ul>
<li>Applications which are computing or processing data with out performing IO operations. (Finance or Trading Applications)</li>
<li>Applications which use cache heavily and don&rsquo;t perform any IO operations</li>
<li>Applications which are asynchronous (i.e. Non Blocking), don&rsquo;t wait on external resources. (Reactive Pattern Applications, NodeJS application)</li>
</ul>


<p>In the above scenarios application is already working in efficiently but in few instances applications with badly written or inefficient code which perform unnecessary heavy calculations or looping on every request tend to show high CPU usage. By profiling application it is easy to figure out the inefficiencies and fix them.</p>

<p>These issues can be fixed by</p>

<ul>
<li>Caching precomputing values</li>
<li>Performing the computation in separate background job.</li>
</ul>


<p><b><i>IO Bound Application</i></b></p>

<p>An application is said to be IO bound if application throughput is limited by its IO or network operations and increasing CPU speed does not bring down application response times. Most applications are IO bound due to the CRUD operation in most applications
Performance tuning or scaling IO bound applications is a difficult job due to its dependency on other systems downstream.</p>

<p>Few scenarios where applications could be IO Bound</p>

<ul>
<li>Applications which are depended on database and perform CRUD operations</li>
<li>Applications which consume drown stream web services for performing its operations</li>
</ul>


<p>Next blog will cover <a href="/2014/05/How-To-Determine-Web-Applications-Thread-Poll-Size/">how to determining optimal thread pool size of an web application</a>.</p>
]]></content>
  </entry>
  
</feed>
