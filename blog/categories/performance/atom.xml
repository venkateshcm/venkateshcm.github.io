<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Performance | Venkatesh CM]]></title>
  <link href="http://venkateshcm.com/blog/categories/performance/atom.xml" rel="self"/>
  <link href="http://venkateshcm.com/"/>
  <updated>2014-06-08T18:20:04+05:30</updated>
  <id>http://venkateshcm.com/</id>
  <author>
    <name><![CDATA[Venkatesh CM]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Web Applications Caching]]></title>
    <link href="http://venkateshcm.com/2014/06/Web-Application-Cache/"/>
    <updated>2014-06-08T00:00:00+05:30</updated>
    <id>http://venkateshcm.com/2014/06/Web-Application-Cache</id>
    <content type="html"><![CDATA[<p>In <a href="/2014/05/Caching-To-Scale-Web-Applications/">Caching To Scale Web Applications</a>, we looked at ways to cache page response to url request by setting http cache headers to enable upstream systems to cache, mostly treating web application as black box. In this post we will look at</p>

<ul>
<li>Cache scopes</li>
<li>Caches Policies</li>
<li>Caching storage strategies and implications on performance</li>
</ul>


<p><b><i>Web Application Caching</i></b></p>

<p>Caching within web application can be done at different scope and granularity.</p>

<p>Some well know (common) ways of caching in web application is based on scope of cached values</p>

<ol>
<li>Session Cache :&ndash; Cached value is stored per key per user and cached values are used for a single user. For example :&ndash; User home country.</li>
<li>Application Cache :&ndash; Cached value is stored per key and cached values are used for multiple users. For example :&ndash; List of countries.</li>
</ol>


<p>Session cache&rsquo;s Time To Live (TTL) is usually in minutes while Application cache&rsquo;s TTL varies widely and usually cached forever until changed with application events or business triggers.</p>

<p>Application cache is memory efficient compared to Session cache, as same cached values is used by multiple users in application cache. But if the cached value is personalized data for a specific user, it should stay in Session Cache.</p>

<p>While Session Cache has been used a lot (sometime abused), Application cache is not widely used.</p>

<p><b><i>Cache Policies</i></b></p>

<ul>
<li><p>Write-through : Cache is updated along with backing datastore synchronously. Since both datastore and cache is always kept in sync, Write-through provides high data integrity and consistency at the cost of performance. Write-through caching makes sense with read heavy applications with very few writes.</p></li>
<li><p>Write-back : Cache is updated synchronously and backing datastore is updated asynchronously. Since only cache is updated synchronously Write-back provides better performance but at the cost of inconsistency or data loss in an event of crash. Write-back caching policy makes sense when there is large number of writes and lossing latest data does not effect application.</p></li>
</ul>


<p>As you can see detecting and handling cached value modification is very important. In fact, it is good practice to make all cached values immutable.</p>

<p><b><i>Cache strategies</i></b></p>

<p>Below are different cache storage options available</p>

<ul>
<li>In-memory cache : cached values are stored in RAM memory.

<ul>
<li>(a) in-process : caching with-in application process</li>
<li>(b) out-of-process : caching in another process</li>
</ul>
</li>
<li>Persistent cache : caching in persistent systems like files or database.</li>
</ul>


<p>To choose caching option, we have to understand performance characteristics of different storage option. Lets start with time taken to perform typical operations on computer. Below numbers are from <a href="http://static.googleusercontent.com/media/research.google.com/en//people/jeff/stanford-295-talk.pdf">Jeff Dean presentation at Google</a>.</p>

<table border="1"><tbody><tr><th colspan='2'><b>Numbers Everyone Should Know:</b></th><tr>
<tr><td> Execute typical instruction :       </td><td style="text-align:right">   1/1,000,000,000 sec = 1 nanosec      </td></tr>
<tr><td> Fetch from L1 cache memory  :        </td><td style="text-align:right">                   &nbsp;0.5 nanosec    </td></tr>
<tr><td> Branch mis-prediction       :        </td><td style="text-align:right">                         5 nanosec      </td></tr>
<tr><td> Fetch from L2 cache memory  :        </td><td style="text-align:right">                         7 nanosec      </td></tr>
<tr><td> Mutex lock/unlock           :        </td><td style="text-align:right">                       100  nanosec      </td></tr>
<tr><td> Fetch from main memory      :        </td><td style="text-align:right">                       100 nanosec      </td></tr>
<tr><td> Compress 1K bytes with Zippy :        </td><td style="text-align:right">                       10,000 nanosec      </td></tr>
<tr><td> Send 2K bytes over 1Gbps network :  </td><td style="text-align:right">                    20,000 nanosec      </td></tr>
<tr><td> Read 1MB sequentially from memory :     </td><td style="text-align:right">                   250,000 nanosec      </td></tr>
<tr><td> Round trip within same datacenter :     </td><td style="text-align:right">                   500,000 nanosec      </td></tr>
<tr><td> Fetch from new disk location (seek) : </td><td style="text-align:right">                 10,000,000 nanosec      </td></tr>
<tr><td> Read 1 MB sequentially from network : </td><td style="text-align:right">                 10,000,000 nanosec      </td></tr>
<tr><td> Read 1MB sequentially from disk : </td><td style="text-align:right">                30,000,000 nanosec      </td></tr>
<tr><td> Send packet CA->Netherlands->CA : </td><td style="text-align:right"> &nbsp;&nbsp;&nbsp;150 millisec = 150,000,000 nanosec      </td></tr>
</tbody>
</table>


<br/>


<p>First thing to notice from above numbers is, L1 and L2 cache. Caching is not only used in applications, network software, database systems, operating systems but also in Computer design.</p>

<br/>


<ul>
<li><p>Back of the Envelope Calculations to determine performance of different caching strategies</p>

<ul>
<li><p>In-memory and in-process caching :&ndash; Cache reads are fetched directly from current process memory.</p>

<p>In-memory and in-process cache fetch time = Fetch from main memory = 100 nanosec</p></li>
<li><p>In-memory and out-of-process caching :&ndash; Cache reads are fetched from another process usually over network. Cached values are stored in another processes memory.</p>

<p>In-memory and out-of-process cache fetch time = Round trip within same datacenter + Fetch from main memory = 500,000 nanosec + 100 nanosec</p></li>
<li><p>Persistent caching :&ndash; Cache reads are read from disk usually over network.</p>

<p>In-memory and out-of-process cache fetch time = Fetch from new disk location (seek) + Round trip within same datacenter = 10,000,000 nanosec + 500,000 nanosec</p>

<p>Persistent caching does not always mean reading from disk, databases usually cache working set data ( often used data ) in its main memory so when there is <b>cache hit</b> it performs similar to In-memory and out-of-process.</p></li>
</ul>
</li>
</ul>


<h6>Summary</h6>


<ul>
<li>In-memory and in-process caching is 500 times faster than In-memory and out-of-process cache fetch time</li>
<li>In-memory and out-of-process cache fetch time is 20 times faster than Persistent caching</li>
</ul>


<p>Clearly in-memory in-process caching is big winner and is widely used in web applications. Does that mean we should always use in-memory in-process caching ? To answer this question we have to understand characteristics of large scale web application.</p>

<p><b><i>Caching in Web Applications Cluster</i></b></p>

<p>As discussed in <a href="/2014/05/Architecture-Issues-Scaling-Web-Applications/">Architecture Issues Scaling Web Applications</a>, large scale web application should be able to</p>

<ul>
<li><p>Horizontal scale out
  We should be able to add identical nodes in each layer to scale web applications.</p></li>
<li><p>No single point of failure
  Is large cluster of nodes, failure of single node can happen and it should not bring down application.</p></li>
</ul>


<p>Due to above two characteristics we end-up with cluster of nodes in any large scale applications.</p>

<p>Getting back to caching, if we go with in-memory in-process caching, each node will have to cache required data. Below are few issues with in-process caching</p>

<ul>
<li>Redundant caching consumes lot of memory. In cluster with N web application processes same cached value has to be stored N times.</li>
<li>Each node will have to face cache miss and perform resource intensive operation before caching. i.e. In N Node cluster resource intensive operation is performed N times. This problem become noticeable if TTL of cached value is low. For Example :&ndash; A web application cluster with 100 nodes, and TTL of 1 min will perform 100 resource intensive operations every minute.</li>
<li>Cache Refresh is another major problem with in-memory in process in cluster of web application process. Refreshing cache across the cluster is not easy and if cache is refreshed based on time. Due to machine time synchronization issue stale cached value will be used in some nodes, depending on application it could cause application in-consistent results based on which node handles request.</li>
</ul>


<p>Out-of-process caching gets around the above issues by storing cached values in distributed caching systems like memcache. Due to central cache handling in out-of-process caching</p>

<ul>
<li>If one of the web application node caches a value, it is available to all other nodes reducing cache misses.</li>
<li>Cache can be refreshed or invalided easily by updating central cache system.</li>
</ul>


<p>Persistent caches is used when it is important to recover from crash with cached values intact. It is achieved by re-loading cached data from disk.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Caching To Scale Web Applications]]></title>
    <link href="http://venkateshcm.com/2014/05/Caching-To-Scale-Web-Applications/"/>
    <updated>2014-05-17T00:00:00+05:30</updated>
    <id>http://venkateshcm.com/2014/05/Caching-To-Scale-Web-Applications</id>
    <content type="html"><![CDATA[<p>In <a href="/2014/05/Architecture-Issues-Scaling-Web-Applications/">Architectural Issues While Scaling Web Applications</a>, I pointed to caching as solution to some of the scaling issues, in this post I will cover different ways of caching and how caching can reduce load on web application and hence improve performance and scalability of web applications. I will cover Web application &amp; database caching in next post.</p>

<p>Caching is the most common way to improve performance or scalability of an web application. In fact, effectiveness of caching is arguably main reason for developer confidence to ignore premature optimisation or to postpone fixing probable performance issue until detecting an issue. The underlying assumption for this confidence is (a) the performance optimisation is may not be required (b) if required, performance issue could be fixed by caching.</p>

<p>Effectiveness of caching in improving performance is due to the fact that most applications are read heavy and perform lot of repetitive operations. So application&rsquo;s performance can be greatly improved by avoiding repeatedly performing resource intensive operation several times. An operation could be resource intensive due to IO activity such as accessing database/service or due to computation/calculations which are CPU intensive.</p>

<p><b><i>What is caching?</i></b></p>

<p>Caching is storing result of an operation which can be used later instead of repeating the operation again.</p>

<p><b><i>Cache hit ratio</i></b></p>

<p>Before executing an operation, a check is performed to determine if the operation result (requested data) is already available in cache. If requested data is available, it is called <b>cache hit</b> and if requested data is not available in cache, it is called <b>cache miss</b>. When cache miss occurs, the requested operation is executed and result is cached for future use (lazy cache population).</p>

<p><b>Cache hit ratio</b> is the number of cache hits divided by total number of requests for an operation. For efficient utilisation of memory, cache hit ration should be high.</p>

<p>Cache hit ratio closer to 0 means most of the requests miss cache and the requested operation is executed every time. Low cache hit ratio could also lead to large cache memory usage as every cache miss could mean new addition to cache storage. Cache hit ratio closer to 1 means most of the requests have cache hits and requested operation is almost never executed.</p>

<p><b><i>Cache population</i></b></p>

<p>Cache could be populated lazily after the executing the operation very first time or by pre-populating cache at the start of application or by another process/background job. Lazy cache population is most common usage pattern but cache population by another background job can be effective when possible.</p>

<p><b><i>Caching Layers in Web Architecture</i></b></p>

<p>There are several layers in typical web application architecture where caching can be performed.</p>

<p><img class="article-img" border="0" src="http://venkateshcm.com/img/blog/CacheArchitecture.png" class="" style="display: inline-block;"></p>

<p>As shown in the above diagram, caching can be done right from browser to database layer of architecture. Let us walk through each layer of caching touching how and what could be cached at that layer and understand pros and cons of caching at each layer.</p>

<p>Few general points to note in the above diagram</p>

<ul>
<li>Caching at left most layer is better for latency.</li>
<li>Caching at right most layers gives better control over granularity of caching and ways to clear or refreshing cache.</li>
</ul>


<p><b><i>Browser Cache</i></b></p>

<p>Browser loads a web page by makes several requests to server for both dynamic resource and static resources like images, style sheets, java scripts etc. Since web application is used by user several times, most of the resources are repeatedly requested from server. Browser can store some of the resources in browser cache and subsequent requests load locally cached resource instead of making server request, reducing the load on web server.</p>

<h6>Cache granularity</h6>


<ul>
<li>Static file caching like images , style sheets, java scripts etc</li>
<li>Browsers also provide local storage (HTML5) and cookies which can be used for caching dynamic data.</li>
</ul>


<h6>How to populate cache</h6>


<ul>
<li>Browser caching works by setting HTTP header parameters like cache control headers of resource to be cached with time to live (TTL).</li>
</ul>


<h6>Regular Cache Refresh</h6>


<ul>
<li>Browser makes server requests after TTL time period and refreshes browser cache.</li>
</ul>


<h6>Forced Cache Refresh</h6>


<ul>
<li>Cache refresh can be forced by changing cache invalidation parameter in the URL of the resource. Cache invalidation can be</li>
<li>a query parameter Eg. <a href="http://server.com/js/library.js?timestamp">http://server.com/js/library.js?timestamp</a></li>
<li>a version number in resource path URL Eg. <a href="http://server.com/v2/js/library.js">http://server.com/v2/js/library.js</a></li>
</ul>


<p>In <a href="http://en.wikipedia.org/wiki/Single-page_application">Single Page Applications (SPA)</a>, the initial page is always loaded from server but all other dependents like images, stylesheets, java scripts etc. are loaded based on  (query parameter or version number in URL). When a new resource or application is deployed to production, cache invalidation parameter is changed to refresh cache.</p>

<h6>Pros</h6>


<ul>
<li>User will experience best response time with no latency.</li>
<li>Can cache both static files and dynamic data.</li>
</ul>


<h6>Cons</h6>


<ul>
<li>Load on server is marginally reduced since repeated requests for the same resource from other machine still hit the server.</li>
<li>Dynamic data stored in HTML5 local storage could be security risk depending on application even through the html6 local storage is accessible to application domain only.</li>
</ul>


<p><b><i>Content Delivery Network(CDN)</i></b></p>

<p><i>Content Delivery Network is a large distributed system of servers deployed in multiple data centres across the Internet. The goal of a CDN is to serve content to end-users with high availability and high performance. CDNs serve a large fraction of the Internet content today, including web objects (text, graphics and scripts), downloadable objects (media files, software, documents), applications (e-commerce, portals), live streaming media, on-demand streaming media, and social networks. </i> &mdash;from <a href="http://en.wikipedia.org/wiki/Content_delivery_network">Wikipedia</a></p>

<p>Content Delivery Network (CDN) providers usually work with Internet Service Providers (ISP) and Telecom companies to add data centres at the last mile of internet connection. Browser request latency is reduced since servers handling requests to cached resource are close to browser client. Browsers usually hold limited number of connection to each host, if the number of requests to a host is more than browser connection limit, requests are queued. CDN caching provides another benefit of distributing resources over several named domains, hence handling more browser requests parallely provides better performance or response time to user. Only static resources are cached in CDN since TTL of resources is usually in the order of days and not seconds.</p>

<h6>Cache granularity</h6>


<ul>
<li>Static File caching like images , style sheets, java scripts etc</li>
</ul>


<h6>How to populate cache</h6>


<ul>
<li>Static resources such as images, stylesheets, java scripts can be stored in CDN using CDN provided tool or API. CDN provides an url for each resource which can be used in web application.</li>
</ul>


<h6>Regular Cache Refresh</h6>


<ul>
<li>CDN tool or API can be used to configure time to live and when to replace static resource on CDN network servers, but replication of CDN resource over distributed network takes time, hence can not be used for very short TTL dynamic resource.</li>
</ul>


<h6>Forced Cache Refresh</h6>


<ul>
<li>Similar to browser cache refresh above, forced cache refresh can be achieved by introducing versioning in url to invalidate cache.</li>
</ul>


<h6>Pros</h6>


<ul>
<li>User will experience good response time with very little latency since resource is returned from nearest server.</li>
<li>Load on server is greatly reduced since all requests for the resource will be handle by CDN.</li>
</ul>


<h6>Cons</h6>


<ul>
<li>Cost of using CDN.</li>
<li>Cache refresh time takes time as it has to refresh on highly distributed network of servers all over the world.</li>
<li>Cache static resources only, since cache refresh is not easy.</li>
</ul>


<p><b><i>Reverse Proxy Cache</i></b></p>

<p>A reverse proxy server is similar to normal proxy server, both act as intermediary between browser client and web server. The main difference is normal proxy server is placed closer to client and reverse proxy server is placed closer to web server. Since requests and responses go through reverse proxy, reverse proxy can cache response to a url and use it to respond to subsequent requests without hitting the web application server. Dynamic resource caching is significant benefit of caching at reverse proxy level with very low TTLs (few seconds). Reverse proxy can cache data in memory or in external cache management tool like memcache, will discuss more on it later.</p>

<p>There are several reverse proxy servers to choose from. Varnish, Squid and Ngnix are some of the options.</p>

<h6>Cache granularity</h6>


<ul>
<li>Static file caching like images, style sheets, java scripts etc</li>
<li>Dynamic page response of http requests.</li>
</ul>


<h6>How to populate cache</h6>


<ul>
<li>Other then reverse proxy server configurations to cache certain resources, proxy servers caching works using HTTP header parameters like cache control, expires headers of resource to be cached with time to leave (TTL).</li>
</ul>


<h6>Regular Cache Refresh</h6>


<ul>
<li>Reverse proxy servers provide ways to invalidate and refresh cache and they make server requests after TTL time period and to refresh cache.</li>
</ul>


<h6>Forced Cache Refresh</h6>


<ul>
<li>Reverse proxy servers can invalidate existing cache on demand.</li>
</ul>


<h6>Pros</h6>


<ul>
<li>Web server load is reduced significantly as multi machine requests can use cache compared to browser cache where only requests for single machine are cached</li>
<li>Reverse proxy servers can cache both static and dynamic resources. Eg. Home page of a site which is same for all clients can be cached for few seconds at reverse proxy.</li>
<li>Most reverse proxy servers act as static file servers as-well, so web application never gets request for static resources.</li>
</ul>


<h6>Cons</h6>


<ul>
<li>User will experience latency since request has to hit distant reverse proxy to get response.</li>
<li>Reverse proxy processing and caching is required, which means added hardware costs compared to CDN costs. But most reverse proxies can be used for multiple purposes -as load balancer, as static file servers as-well as caching layer, added cost of additional box should not be a problem.</li>
</ul>


<p>Next part of this post will cover different ways of <a href="/2014/06/Web-Application-Cache/">caching within web application</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How To Determine Web Application Thread Pool Size]]></title>
    <link href="http://venkateshcm.com/2014/05/How-To-Determine-Web-Applications-Thread-Poll-Size/"/>
    <updated>2014-05-10T00:00:00+05:30</updated>
    <id>http://venkateshcm.com/2014/05/How-To-Determine-Web-Applications-Thread-Poll-Size</id>
    <content type="html"><![CDATA[<p>Continuing on <a href="/2014/05/Architecture-Issues-Scaling-Web-Applications/">Architectural Issues faced while scaling web applications</a>, in this blog I will cover a common issue, how to determine web application thread pool size?, that shows up while deploying web applications to production or while performance testing web applications.</p>

<p><b><i>Thread Pool</i></b></p>

<p>In web applications thread pool size determines the number of concurrent requests that can be handled at any given time. If a web application gets more requests than thread pool size, excess requests are either queued or rejected.</p>

<p>Please note concurrent is not same as parallel. Concurrent requests are number of requests being processed while only few of them could be running on CPUs at any point of time. Parallel requests are number of requests being processed while all of them are running on CPUs at any point of time.</p>

<p>In Non-blocking IO applications such as NodeJS, a single thread (process) can handles multiple requests concurrently. In multi-core CPUs boxes, parallel requests can be handled by increasing number of threads or processes.</p>

<p>In blocking IO applications such as Java SpringMVC, a single thread can handle only one request concurrently. To handle more than one request concurrently we have to increase the number of threads.</p>

<p><b><i>CPU Bound Applications</i></b></p>

<p> In CPU bound applications thread Pool size should be equal number of cpus on the box. Adding more threads would interrupt request processing due to thread context switching and also increases response time.</p>

<p> Non-blocking IO applications will be CPU bound as there are no thread wait time while requests get processed.</p>

<p><b><i>IO Bound Applications</i></b></p>

<p>Determining thread pool size of IO bound application is lot more complicated and depends on response time of down stream systems, since a thread is blocked until other system responds. We would have to increase the number of threads to better utilise CPU as discussed in <a href="/2014/04/Reactor-Pattern-Part-1-Non-blocking-I-O/">Reactor Pattern Part 1 : Applications with Blocking I/O</a>.</p>

<p><b><i>Little&rsquo;s law</i></b></p>

<p>Little&rsquo;s law was used in non technology fields like banks to figure out number of bank teller counters required to handle incoming bank customers.</p>

<p><b><a href="http://en.wikipedia.org/wiki/Little's_law">Little&rsquo;s law</a></b></p>

<pre><code>The long-term average number of customers in a stable system L is equal to the long-term 
average effective arrival rate, λ, multiplied by the average time a customer spends 
in the system, W; or expressed algebraically: 
L = λW.
</code></pre>

<p><b>Little&rsquo;s law applied to web applications</b></p>

<pre><code>The average number of threads in a system (Threads) is equal average web request 
arrival rate (WebRequests per sec), multiplied by the average response time (ResponseTime)
</code></pre>

<p>Threads = Number of Threads <br/>
WebRequests per sec = Number of Web Requests that can be processed in one second <br/>
ResponseTime = Time taken to process one web request <br/></p>

<pre><code>Threads = (WebRequests/sec) X ResponseTime
</code></pre>

<p>While the above equation provides the number of threads required to handle incoming requests, it does not provide information on the threads to cpu ratio i.e. how many threads should be allocated on a given box with x CPUs.</p>

<p><b><i>Testing to determining Thread Pool size</i></b></p>

<p>To find right thread pool size is to balance between throughput and response time. Starting with minimum a thread per cpu (Threads Pool Size = No of CPUs) , application thread pool size is directly proportional to the average response time of down stream systems until CPU usage is maxed out or response time is not degraded.</p>

<p>Below diagrams illustrate how number of requests, CPU and Response Time metrics are connected.</p>

<p>CPU Vs Number of Requests graph shows how CPU usage while increasing load on the web applications.</p>

<p>Response Time Vs Number of Requests graph shows response time impact due to increasing load on the web applications.</p>

<p>Green dot indicates point of optimal throughput and response time.</p>

<p><b>Thread pool size = Number of CPUs</b></p>

<p><img class="article-img" border="0" height="325" width="480" src="http://venkateshcm.com/img/blog/MinimumThreads.png" class="" style="display: inline-block;"></p>

<p>Above diagram depicts blocking IO bound applications when number of threads is equal to number cpus.
Application threads get blocked waiting for down stream systems to respond. Response time increases as requests get queued since threads are blocked. Application starts rejecting requests as all threads are blocked even though CPU usage is very low.</p>

<p><b>Large Thread pool size</b></p>

<p><img class="article-img" border="0" height="325" width="480" src="http://venkateshcm.com/img/blog/MaximumThreads.png" class="" style="display: inline-block;"></p>

<p>Above diagram depicts blocking IO bound applications when large number of threads are created in web application. Due to large number of threads, thread context switching will be very frequent. Application CPU usage gets maxed out even though throughput has not increased due to unnecessary thread context switching. Response Time degrades since requests are interrupted with context switching.</p>

<p><b>Optimal Thread pool size</b></p>

<p><img class="article-img" border="0" height="325" width="480" src="http://venkateshcm.com/img/blog/OptimalThreads.png" class="" style="display: inline-block;"></p>

<p>Above diagram depicts blocking IO bound applications when optimal number of threads are created in web application. CPU gets efficiently used with good throughput and fewer thread context switching. We notice good response time due to efficient request processing with fewer interrupts (context switching).</p>

<p><b><i>Thread Pool isolation</i></b></p>

<p>In most web applications, few types of web request take much longer to process than other web request types.The slower web requests might hog all threads and bring down entire application.</p>

<p>Couple of ways to handle this issue is</p>

<ul>
<li>to have separate box to handle slow web requests.</li>
<li>to allocate a separate thread pool for slow web requests within the same application.</li>
</ul>


<p>Determining optimal thread pool size of a blocking IO web application is difficult task. Usually done by performing several performance runs. Having several thread pools in a web application further complicates the process of determining optimal thread pool size.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Architecture Issues Scaling Web Applications]]></title>
    <link href="http://venkateshcm.com/2014/05/Architecture-Issues-Scaling-Web-Applications/"/>
    <updated>2014-05-05T00:00:00+05:30</updated>
    <id>http://venkateshcm.com/2014/05/Architecture-Issues-Scaling-Web-Applications</id>
    <content type="html"><![CDATA[<p>I will cover architecture issues that show up while scaling and performance tuning large scale web application in this blog.</p>

<p>Lets start by defining few terms to create common understanding and vocabulary. Later on I will go through different issues that pop-up while scaling web application like</p>

<ul>
<li>Architecture bottlenecks</li>
<li>Scaling Database</li>
<li>CPU Bound Application</li>
<li>IO Bound Application</li>
</ul>


<p><a href="/2014/05/How-To-Determine-Web-Applications-Thread-Poll-Size/">Determining optimal thread pool size of an web application</a> will be covered in next blog.</p>

<p><b><i>Performance</i></b></p>

<p>Term performance of web application is used to mean several things. Most developers are primarily concerned with response time and scalability.</p>

<ul>
<li><p><h5>Response Time</h5></p>

<p>Is the time taken by web application to process request and return response. Applications should respond to requests (response time) within acceptable duration. If application is taking beyond the acceptable time, it is said to be non-performing or degraded.</p></li>
<li><p><h5>Scalability</h5></p>

<p>Web application is said to be scalable if by adding more hardware, application can linearly take more requests than before. Two ways of adding more hardware are</p>

<ul>
<li><b>Scaling Up (vertical scaling)</b> :&ndash;
increasing the number CPUs or adding faster CPUs on a single box.</li>
<li><b>Scaling Out (horizontal scaling)</b> :&ndash;
increasing the number of boxes.</li>
</ul>
</li>
</ul>


<p><b><i>Scaling Up Vs Scaling Out</i></b></p>

<p>Scaling out is considered more important as commodity hardware is cheaper compared to cost of special configuration hardware (super computer). But increasing the number of requests that an application can handle on a single commodity hardware box is also important. An application is said to be performing well if it can handle more requests with-out degrading response time by just adding more resources.</p>

<p><b><i>Response time Vs Scalability</i></b></p>

<p>Response time and Scalability don&rsquo;t always go together i.e. application might have acceptable response times but may not handle more than certain number of requests or application can handle increasing number of requests but has poor or long response times. We have to strike a balance between scalability and response time to get good performance of the application.</p>

<p><b><i>Capacity Planning</i></b></p>

<p>Capacity planning is an exercise of figuring out the required hardware to handle expected load in production. Usually it involves figuring out performance of application with fewer boxes and based on performance per box projecting it. Finally verifying it with load/performance tests.</p>

<p><b><i>Scalable Architecture</i></b></p>

<p>Application architecture is scalable if each layer in multi layered architecture is scalable (scale out). For example :&ndash; As shown in diagram below we should be able linearly scale by add additional box in Application Layer or Database Layer.</p>

<p><img class="article-img" border="0" height="325" width="480" src="http://venkateshcm.com/img/blog/scaling.png" class="" style="display: inline-block;"></p>

<p><b><i>Scaling Load Balancer</i></b></p>

<p>Load balancers can be scaled out by point DNS to multiple IP addresses and using DNS Round Robin for IP address lookup. Other option is to front another load balancer which distributes load to next level load balancers.</p>

<p>Adding multiple Load balancers is rare as a single box running nginx or HAProxy can handle more than 20K concurrent connections per box compared to web application boxes which can handle few thousand concurrent requests. So a single load balancer box can handle several web application boxes.</p>

<p><b><i>Scaling Database</i></b></p>

<p>Scaling database is one of the most common issues faced. Adding business logic (stored procedure, functions) in database layer brings in additional overhead and complexity.</p>

<p><b>RDBMS</b></p>

<p>RDBMS database can be scaled by having master-slave mode with read/writes on master database and only reads on slave databases. Master-Slave provides limited scaling of reads beyond which developers has to split the database into multiple databases.</p>

<p><b>NoSQL</b></p>

<p><a href="http://en.wikipedia.org/wiki/CAP_theorem">CAP theorem</a> has shown that is not possible to get Consistency, Availability and Partition tolerance simultaneously. NoSql databases usually compromise on consistency to get high availability and partition.</p>

<p><b>Splitting database</b></p>

<p>Database can be split vertically (Partitioning) or horizontally (Sharding).</p>

<ul>
<li><p>Vertically splitting (Partitioning) :&ndash; Database can be split into multiple loosely coupled sub-databases based of domain concepts. Eg:&ndash; Customer database, Product Database etc. Another way to split database is by moving few columns of an entity to one database and few other columns to another database. Eg:&ndash; Customer database , Customer contact Info database, Customer Orders database etc.</p></li>
<li><p>Horizontally splitting (Sharding) :&ndash; Database can be horizontally split into multiple database based on some discrete attribute. Eg:&ndash;  American Customers database, European Customers database.</p></li>
</ul>


<p>Transiting from single database to multiple database using partitioning or sharding is a challenging task.</p>

<p><b><i>Architecture bottlenecks</i></b></p>

<p>Scaling bottlenecks are formed due to two issues</p>

<ul>
<li><p><b>Centralised component</b>
A component in application architecture which can not be scaled out adds an upper limit on number of requests that entire architecture or request pipeline can handle.</p></li>
<li><p><b>High latency component</b>
A slow component in request pipeline puts lower limit on the response time of the application. Usual solution to fix this issue is to make high latency components into background jobs or executing them asynchronously with queuing.</p></li>
</ul>


<p><b><i>CPU Bound Application</i></b></p>

<p>An application is said to be CPU bound if application throughput is limited by its CPU. By increasing CPU speed application response time can be reduced.</p>

<p>Few scenarios where applications could be CPU Bound</p>

<ul>
<li>Applications which are computing or processing data with out performing IO operations. (Finance or Trading Applications)</li>
<li>Applications which use cache heavily and don&rsquo;t perform any IO operations</li>
<li>Applications which are asynchronous (i.e. Non Blocking), don&rsquo;t wait on external resources. (Reactive Pattern Applications, NodeJS application)</li>
</ul>


<p>In the above scenarios application is already working in efficiently but in few instances applications with badly written or inefficient code which perform unnecessary heavy calculations or looping on every request tend to show high CPU usage. By profiling application it is easy to figure out the inefficiencies and fix them.</p>

<p>These issues can be fixed by</p>

<ul>
<li>Caching precomputed values</li>
<li>Performing the computation in separate background job.</li>
</ul>


<p>Different ways of caching, how caching can reduce load and improve performance and scalability of web applications is covered in post <a href="/2014/05/Caching-To-Scale-Web-Applications/">Caching To Scale Web Applications</a></p>

<p><b><i>IO Bound Application</i></b></p>

<p>An application is said to be IO bound if application throughput is limited by its IO or network operations and increasing CPU speed does not bring down application response times. Most applications are IO bound due to the CRUD operation in most applications
Performance tuning or scaling IO bound applications is a difficult job due to its dependency on other systems downstream.</p>

<p>Few scenarios where applications could be IO Bound</p>

<ul>
<li>Applications which are depended on database and perform CRUD operations</li>
<li>Applications which consume drown stream web services for performing its operations</li>
</ul>


<p>Next blog will cover <a href="/2014/05/How-To-Determine-Web-Applications-Thread-Poll-Size/">how to determining optimal thread pool size of an web application</a>.</p>
]]></content>
  </entry>
  
</feed>
